# verl GRPO VLM training config on Geometry3K dataset.
#
# EXPERIMENTAL: This config is not yet fully tested.
#
# Note: the original model is not compatible with the latest version of transformers and oumi
# We use the oumi-ai version of the model instead until the original model is updated.
#
# Requirements:
#   - uv pip install einops tf-keras
#
# Usage:
#   oumi train -c configs/recipes/vision/molmo/grpo/train.yaml
#
# See Also:
#   - Documentation: https://oumi.ai/docs/en/latest/user_guides/train/train.html
#   - Config class: oumi.core.configs.TrainingConfig
#   - Config source: https://github.com/oumi-ai/oumi/blob/main/src/oumi/core/configs/training_config.py
#   - Other training configs: configs/**/*train.yaml

model:
  # model_name: "allenai/Molmo-7B-O-0924"  # original model is not compatible with the latest version of transformers
  model_name: "oumi-ai/Molmo-7B-O-0924"
  torch_dtype_str: "float32"
  trust_remote_code: True

data:
  train:
    datasets:
      - dataset_name: "hiyouga/geometry3k"
        split: "train"
        shuffle: True
        seed: 42
        transform_num_workers: "auto"
        dataset_kwargs:
          processor_name: "oumi-ai/Molmo-7B-O-0924"
          # processor_name: "allenai/Molmo-7B-O-0924"
          return_conversations: True

  validation:
    datasets:
      - dataset_name: "hiyouga/geometry3k"
        split: "validation"
        shuffle: True
        seed: 42
        transform_num_workers: "auto"
        dataset_kwargs:
          processor_name: "oumi-ai/Molmo-7B-O-0924"
          # processor_name: "allenai/Molmo-7B-O-0924"
          return_conversations: True

training:
  output_dir: "output/grpo-verl-molmo-o"
  enable_wandb: True

  trainer_type: "VERL_GRPO"
  num_train_epochs: 1
  save_steps: 150

  learning_rate: 1.0e-6
  enable_gradient_checkpointing: False

  # No custom reward function forces verl to use the default built-in reward
  # function pre-configured for the "hiyouga/geometry3k" data source:
  # https://github.com/volcengine/verl/blob/main/verl/utils/reward_score/geo3k.py#L31
  reward_functions: []

  # The parameters in "grpo:" and "verl_config_overrides:" sections are set to match
  # the Qwen2.5 VL example provided in the `verl` repository:
  # https://github.com/volcengine/verl/blob/main/examples/grpo_trainer/run_qwen2_5_vl-7b.sh
  grpo:
    max_completion_length: 1024
    use_vllm: True
    temperature: 0.6
    vllm_gpu_memory_utilization: 0.6

  # For details on `verl` parameters, please refer to documentation:
  # https://verl.readthedocs.io/en/latest/examples/config.html
  verl_config_overrides:
    data:
      train_batch_size: 512
      val_batch_size: 512
      max_prompt_length: 1024
      max_response_length: 2048
      filter_overlong_prompts: True
      truncation: "error"
      image_key: "images"
    algorithm:
      use_kl_in_reward: False
    actor_rollout_ref:
      model:
        use_remove_padding: True
      actor:
        ppo_mini_batch_size: 128
        ppo_micro_batch_size_per_gpu: 10
        use_kl_loss: True
        kl_loss_coef: 0.01
        kl_loss_type: "low_var_kl"
        entropy_coeff: 0
        param_offload: False
        optimizer_offload: False
        fsdp_config:
          param_offload: False
          optimizer_offload: False
      rollout:
        tensor_model_parallel_size: 2
        log_prob_micro_batch_size_per_gpu: 20
        n: 5
        enforce_eager: False
        free_cache_engine: False
        enable_chunked_prefill: False
      ref:
        log_prob_micro_batch_size_per_gpu: 20
        fsdp_config:
          param_offload: True
    trainer:
      n_gpus_per_node: 4
      nnodes: 1
