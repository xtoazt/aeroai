# Polaris job config for vLLM inference.
#
# Usage:
#   oumi launch up -c configs/examples/misc/vllm_polaris_job.yaml --cluster debug.$ALCF_USER --user $ALCF_USER
#
# See Also:
#   - Documentation: https://oumi.ai/docs/en/latest/user_guides/launch/launch.html
#   - Config class: oumi.core.configs.JobConfig
#   - Config source: https://github.com/oumi-ai/oumi/blob/main/src/oumi/core/configs/job_config.py
#   - Other job configs: configs/**/*job.yaml

name: vllm
# NOTE: Replace with your username.
user: your_username
num_nodes: 1

resources:
  cloud: polaris

# Upload working directory to ~/oumi_launcher/{submission_time}
working_dir: .

# NOTE: Replace with your input/output paths, and adjust other parameters as needed.
envs:
  REPO: meta-llama  # HF model repo.
  MODEL: Llama-3.3-70B-Instruct  # HF model name.
  OUMI_VLLM_INPUT_FILEPATH: your_input_filepath  # Path to input data file.
  OUMI_VLLM_OUTPUT_DIR: your_output_dir  # Output dir.
  OUMI_VLLM_NUM_WORKERS: 100  # Samples will be divided amongst workers
  OUMI_VLLM_WORKERS_SPAWNED_PER_SECOND: 10

setup: |
  #PBS -l place=scatter
  #PBS -l walltime=00:40:00
  #PBS -l filesystems=home:eagle
  #PBS -A community_ai
  #PBS -o /eagle/community_ai/jobs/logs
  #PBS -e /eagle/community_ai/jobs/logs

run: |
  set -e

  # Change to the directory where the job was submitted.
  echo "Changing directory to ${PBS_O_WORKDIR} ..."
  cd ${PBS_O_WORKDIR}

  # Run the job.
  ./scripts/polaris/jobs/vllm_job.sh
